# ğŸš¨ Silent Data Drift Detector for Startups

A lightweight, production-friendly system to **detect silent data drift** in machine learning pipelines **before model performance degrades**.

---

## â“ Why This Project?

Startups often deploy ML models quickly, but over time:

- User behavior changes
- Feature distributions shift
- Model accuracy drops **silently**
- Labels are unavailable in production

Most teams notice only **after business damage occurs**.

Existing drift tools are often **heavy, expensive, or overkill** for early-stage startups.

---

## ğŸ’¡ Solution Overview

This project implements a **lightweight data drift monitoring system** that:

- Compares **incoming live data** with **training-time baseline data**
- Detects **statistical distribution drift**
- Works **without labels**
- Requires **no retraining**
- Has **no cloud dependency**

It provides **early warnings** so teams can act *before* models fail.

---

## ğŸ§  Key Idea

Instead of monitoring model accuracy (which requires labels),  
we monitor **data distributions** directly using statistical metrics.

> If the data changes, the model is at risk â€” even if accuracy hasnâ€™t dropped yet.

---

## ğŸ—ï¸ System Architecture

Raw Historical Data
â†“
Baseline Statistics (Mean, Variance, PSI bins)
â†“
Live Data (Simulated as Batches)
â†“
Drift Detection Engine
â†“
Early Warnings (Console / Logs)


---

## ğŸ“Š Drift Detection Metrics Used

For each numeric feature, the system monitors:

- **Mean Shift** â€“ relative change in average
- **Variance Shift** â€“ spread change
- **Population Stability Index (PSI)** â€“ distributional shift  
  (implemented using **percentile-based bins**, industry standard)

### PSI Interpretation
- `PSI < 0.10` â†’ No drift  
- `0.10 â‰¤ PSI < 0.25` â†’ Moderate drift  
- `PSI â‰¥ 0.25` â†’ Severe drift  

---

## âš ï¸ Important Design Decisions

- **Timestamps are excluded** from drift detection  
  (they are monotonically increasing and always drift by definition)

- **No labels are used**  
  (reflects real production constraints)

- **Baseline statistics are stored**, not raw training data  
  (memory-efficient and reusable)

---

## ğŸ§ª Drift Simulation (Validation)

To validate the system, **controlled drift** is injected into live data:

- Transaction amounts are artificially scaled (e.g., 2Ã— increase)
- Early batches show **no drift**
- Later batches correctly trigger **SEVERE DRIFT alerts**

This demonstrates that the system:
- Avoids false positives
- Detects real distribution changes reliably

---

## ğŸ“ Project Structure

```text
silent-data-drift-detector/
â”œâ”€â”€ compute_baseline_stats.py     # Compute baseline feature statistics
â”œâ”€â”€ create_live_batches.py        # Simulate streaming data
â”œâ”€â”€ split_data.py                 # Time-based train/live split
â”œâ”€â”€ inject_drift.py               # Inject artificial drift
â”œâ”€â”€ psi.py                        # Percentile-based PSI implementation
â”œâ”€â”€ drift_detector.py             # Core drift detection logic
â”œâ”€â”€ monitor.py                    # Monitor live batches
â”œâ”€â”€ baseline_stats.json           # Stored baseline reference
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

---

â–¶ï¸ How to Run

1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Prepare Baseline & Live Data
python split_data.py
python create_live_batches.py

3ï¸âƒ£ Compute Baseline Statistics
python compute_baseline_stats.py

4ï¸âƒ£ (Optional) Inject Drift
python inject_drift.py

5ï¸âƒ£ Monitor for Drift
python monitor.py

ğŸ“Œ Example Output
ğŸ“¦ Monitoring live_batch_25.csv
âš ï¸ amt | PSI=0.58 | MeanShift=1.01 | VarShift=3.57 | SEVERE DRIFT

ğŸ¯ Why Startups Would Use This

ğŸš« Prevents silent ML failures
ğŸ’¸ Saves customer trust & revenue
ğŸ§  Encourages proactive ML monitoring
âš¡ Lightweight & low infrastructure cost
ğŸ”Œ Easy to integrate into existing pipelines

ğŸ›  Tech Stack

Python
pandas, numpy
scipy
No cloud services
No MLOps frameworks

ğŸ“Œ Future Enhancements

Streamlit dashboard for visual monitoring
Logging / email alerts
Categorical feature drift detection
Integration with CI/CD or model registries